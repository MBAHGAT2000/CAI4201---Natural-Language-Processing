{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15829a43",
   "metadata": {},
   "source": [
    "# <h1 align=center>**Text Normalization**</h1>\n",
    "It involves cleaning and preprocessing text data to make it consistent and usable for different NLP tasks. The process includes a variety of techniques, such as case normalization, punctuation removal, stop word removal, stemming, and lemmatization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9a320d-2761-4dd1-83a3-021f45e7bd31",
   "metadata": {},
   "source": [
    "- <a href='#Tokenization'>Tokenization</a>\n",
    "    - <a href='#Tokenization Using NLTK EN'>1.1. Tokenization Using NLTK EN</a>\n",
    "    - <a href='#Tokenization Using NLTK AR'>1.2. Tokenization Using NLTK AR</a>\n",
    "- <a href='#POS'>2. POS</a>\n",
    "- <a href='#NER'>3. NER</a>\n",
    "- <a href='#Text Normalizaion ( Cleaning )'>Text Normalizaion ( Cleaning )</a>\n",
    "    - <a href='#stop words in EN'>4.1.  Stop words in EN</a>\n",
    "    - <a href='#punctuation'>4.2.  Punctuation</a>\n",
    "    - <a href='#stemming using nltk'>4.3.  Stemming using nltk</a>\n",
    "    - <a href='#stemming in AR'>4.4.  Stemming in AR</a>\n",
    "    - <a href='#lemmatization using nltk'>4.5.  Lemmatization using nltk</a>\n",
    "    - <a href='#lemmatization in AR'>4.6.  Lemmatization in AR</a>\n",
    "    - <a href='#special arabic cleaning functions'>4.7.  Special arabic cleaning functions</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9cefac-ddea-4bc2-aed0-18c1f482c63f",
   "metadata": {},
   "source": [
    "<h1 align=center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620d9926-6287-4501-869e-ae263661f640",
   "metadata": {},
   "source": [
    "# Import Library ( nltk )\n",
    "- work with human language data for applying in statistical natural language processing (NLP).\n",
    "- It contains text processing libraries for tokenization, parsing, classification, stemming, tagging and semantic reasoning\n",
    "- Ref : \"https://www.nltk.org/data.html\"\n",
    "- NLTK **Corpora ( DataSets )** : \"https://www.nltk.org/nltk_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "32af75b5-ddcb-4858-bf06-414887cc07ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download() # to download all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ed50c07-fb2b-4330-868a-3d1866b37748",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ARLSTem',\n",
       " 'ARLSTem2',\n",
       " 'AbstractLazySequence',\n",
       " 'AffixTagger',\n",
       " 'AlignedSent',\n",
       " 'Alignment',\n",
       " 'AnnotationTask',\n",
       " 'ApplicationExpression',\n",
       " 'Assignment',\n",
       " 'BigramAssocMeasures']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to know the functions in the library\n",
    "dir(nltk)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6386a0f5-990f-4ecf-9610-cdb19813078a",
   "metadata": {},
   "source": [
    "<b>\n",
    "<a id='Tokenization'></a>\n",
    "<font size=\"7\">Tokenization</font>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0b3c2c-2577-493c-9371-5333d20d08b4",
   "metadata": {},
   "source": [
    "##### Tokenization : mean split the sentence into tokens( **words** ) and called **Word tokenizer**\n",
    "##### Tokenization : split paragraph into sentence and called **sentence tokenizer**\n",
    "- Hint  Word tokenizer : is like split but split depend on space or symbol to seprate words\n",
    "- Apply tokenization Text ->  **This product is amazing, but the delivery was late.**\n",
    "  - **Answer** :  [\"This\", \"product\", \"is\", \"amazing\", \",\", \"but\", \"the\", \"delivery\", \"was\", \"late\", \".\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d79b54-4203-45be-bb62-e6bc9eeab44a",
   "metadata": {},
   "source": [
    "<b>\n",
    "<a id='Tokenization Using NLTK EN'></a>\n",
    "<font size=\"5\">Tokenization Using NLTK EN</font>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "61a61507-7f67-4f60-8cf0-c7e82c3c2e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "#word_tokenize is a function\n",
    "EXAMPLE_TEXT = \"\"\"\n",
    "Hello Mr. Smith, how are you doing today? The weather is great,\n",
    "and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\n",
    "\"\"\"\n",
    "print(word_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8838ead-922b-43a5-89a7-00fe06ed9640",
   "metadata": {},
   "source": [
    "#### Difference between split and token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "cd94282d-1b33-445d-9671-9015a963a792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', \"name's\", 'hossam', 'and', 'i', 'work', 'as', 'a', 'teacher', 'assistant', 'at', 'bfcai', 'and', 'i', 'have', '600$']\n",
      "------------------------\n",
      "['my', 'name', \"'s\", 'hossam', 'and', 'i', 'work', 'as', 'a', 'teacher', 'assistant', 'at', 'bfcai', 'and', 'i', 'have', '600', '$']\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "text = \"my name's hossam and i work as a teacher assistant at bfcai and i have 600$\"\n",
    "print(text.split())\n",
    "print('------------------------')\n",
    "print(word_tokenize(text))\n",
    "print('====================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9f5d4d3f-0ef1-428c-9f60-09f3d57a4516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good muffins cost $3.88\\nin New York.', 'Please buy me two of them.', 'Thanks.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "s = 'Good muffins cost $3.88\\nin New York.  Please buy me two of them.\\n\\nThanks.'\n",
    "print(sent_tokenize(s))\n",
    "#whitespaceTokenizer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bdc561-27ff-4e56-b56b-8065502827f7",
   "metadata": {},
   "source": [
    "<b>\n",
    "<a id='Tokenization Using NLTK AR'></a>\n",
    "<font size=\"5\">Tokenization Using NLTK AR</font>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "359b665a-4457-49fd-8508-0c5b24047a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['هل', 'تعلم؟', '#', 'نيوكاسل', 'يتفوق', 'بالمواجهات', 'المباشرة', 'على', '#', 'ارسنال', 'في', 'تاريخ', 'الدوري', 'الممتاز', 'الانجليزي؟', 'بعد', 'قليل', 'أرقام', 'واحصائيات', 'للفريقين']\n"
     ]
    }
   ],
   "source": [
    "EXAMPLE_TEXT='هل تعلم؟ #نيوكاسل يتفوق بالمواجهات المباشرة على #ارسنال في تاريخ الدوري الممتاز الانجليزي؟ بعد قليل أرقام واحصائيات للفريقين'\n",
    "print(word_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7c4e1ba9-cbbd-40c3-bcdb-8622415ae3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['هل', 'تعلم؟', '#نيوكاسل', 'يتفوق', 'بالمواجهات', 'المباشرة', 'على', '#ارسنال', 'في', 'تاريخ', 'الدوري', 'الممتاز', 'الانجليزي؟', 'بعد', 'قليل', 'أرقام', 'واحصائيات', 'للفريقين']\n",
      "------------------------\n",
      "['هل', 'تعلم؟', '#', 'نيوكاسل', 'يتفوق', 'بالمواجهات', 'المباشرة', 'على', '#', 'ارسنال', 'في', 'تاريخ', 'الدوري', 'الممتاز', 'الانجليزي؟', 'بعد', 'قليل', 'أرقام', 'واحصائيات', 'للفريقين']\n",
      "====================================================\n"
     ]
    }
   ],
   "source": [
    "print(EXAMPLE_TEXT.split())\n",
    "print('------------------------')\n",
    "print(word_tokenize(EXAMPLE_TEXT))\n",
    "print('====================================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5dde51c",
   "metadata": {},
   "source": [
    "<b>\n",
    "<a id='POS'></a>\n",
    "<font size=\"5\">POS</font>\n",
    "</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24c7ba8",
   "metadata": {},
   "source": [
    "- POS Tagging **(Parts of Speech Tagging)**\n",
    "    - is a process to mark up the words in text format for a particular part of a speech based on its definition and context.\n",
    "    - It is responsible for text reading in a language and assigning some specific token (Parts of Speech) to each word.\n",
    "    - It is also called grammatical tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "812071c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''' POS Tagging (Parts of Speech Tagging) is a process to mark up the words in text format for a particular part of a speech based on its definition and context.\n",
    "It is responsible for text reading in a language and assigning some specific token (Parts of Speech) to each word. It is also called grammatical tagging.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "438cf0b6-e479-424d-baac-5916d8d5dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bd49261f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('POS', 'NNP'), ('Tagging', 'NNP'), ('(', '('), ('Parts', 'NNP'), ('of', 'IN'), ('Speech', 'NNP'), ('Tagging', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('process', 'NN'), ('to', 'TO'), ('mark', 'VB'), ('up', 'RP'), ('the', 'DT'), ('words', 'NNS'), ('in', 'IN'), ('text', 'JJ'), ('format', 'NN'), ('for', 'IN'), ('a', 'DT'), ('particular', 'JJ'), ('part', 'NN'), ('of', 'IN'), ('a', 'DT'), ('speech', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('its', 'PRP$'), ('definition', 'NN'), ('and', 'CC'), ('context', 'NN'), ('.', '.'), ('It', 'PRP'), ('is', 'VBZ'), ('responsible', 'JJ'), ('for', 'IN'), ('text', 'JJ'), ('reading', 'NN'), ('in', 'IN'), ('a', 'DT'), ('language', 'NN'), ('and', 'CC'), ('assigning', 'VBG'), ('some', 'DT'), ('specific', 'JJ'), ('token', 'NN'), ('(', '('), ('Parts', 'NNP'), ('of', 'IN'), ('Speech', 'NNP'), (')', ')'), ('to', 'TO'), ('each', 'DT'), ('word', 'NN'), ('.', '.'), ('It', 'PRP'), ('is', 'VBZ'), ('also', 'RB'), ('called', 'VBN'), ('grammatical', 'JJ'), ('tagging', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "print(tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1b692d",
   "metadata": {},
   "source": [
    "\n",
    " # Abbreviation\tMeaning\n",
    " The below NLTK POS tag list contains all the NLTK POS Tags. NLTK POS tagger is used to assign grammatical information of each word of the sentence. Installing, Importing and downloading all the packages of POS NLTK is complete.\n",
    "- CC\t\tcoordinating conjunction\n",
    "- CD\t\tcardinal digit\n",
    "- DT\t\tdeterminer\n",
    "- EX\texistential there\n",
    "- FW\tforeign word\n",
    "- IN\tpreposition/subordinating conjunction\n",
    "- JJ\tThis NLTK POS Tag is an adjective (large)\n",
    "- JJR\tadjective, comparative (larger)\n",
    "- JJS\tadjective, superlative (largest)\n",
    "- LS\tlist market\n",
    "- MD\tmodal (could, will)\n",
    "- NN\tnoun, singular (cat, tree)\n",
    "- NNS\tnoun plural (desks)\n",
    "- NNP\tproper noun, singular (sarah)\n",
    "- NNPS\tproper noun, plural (indians or americans)\n",
    "- PDT\tpredeterminer (all, both, half)\n",
    "- POS\tpossessive ending (parent\\ ‘s)\n",
    "- PRP\tpersonal pronoun (hers, herself, him, himself)\n",
    "- PRP$\tpossessive pronoun (her, his, mine, my, our )\n",
    "- RB\tadverb (occasionally, swiftly)\n",
    "- RBR\tadverb, comparative (greater)\n",
    "- RBS\tadverb, superlative (biggest)\n",
    "- RP\tparticle (about)\n",
    "- TO\tinfinite marker (to)\n",
    "- UH\tinterjection (goodbye)\n",
    "- VB\tverb (ask)\n",
    "- VBG\tverb gerund (judging)\n",
    "- VBD\tverb past tense (pleaded)\n",
    "- VBN\tverb past participle (reunified)\n",
    "- VBP\tverb, present tense not 3rd person singular(wrap)\n",
    "- VBZ\tverb, present tense with 3rd person singular (bases)\n",
    "- WDT\twh-determiner (that, what)\n",
    "- WP\twh- pronoun (who)\n",
    "-WRB\twh- adverb (how) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620835f0",
   "metadata": {},
   "source": [
    "<b>\n",
    "<a id='NER'></a>\n",
    "<font size=\"5\">NER</font>\n",
    "</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7286fce5",
   "metadata": {},
   "source": [
    "- The term Named Entity : to identify **names of organisations, people and geographic locations** in the text, currency, time, and percentage expressions.\n",
    "- Today, NER is widely used across various fields and sectors to automate the information extraction process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "959d1025",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"WASHINGTON -- In the wake of a string of abuses by New York police officers in the 1990s, Loretta E. Lynch, the top federal prosecutor in Brooklyn, spoke forcefully about the pain of a broken trust that African-Americans felt and said the responsibility for repairing generations of miscommunication and mistrust fell to law enforcement.\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tagged = nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c38508d4-335c-4cbf-8b82-e2ee932b9af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('WASHINGTON', 'NNP')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged[0] # Return : (\"WORD\" , \"PART OF SPEECH\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8257f4e3-3916-4068-9d72-4cb57a4aa755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('WASHINGTON', 'NNP')"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged[0][0]   , tagged[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9ae4185c-aef3-4599-9a70-5cab93dc6d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('WASHINGTON', 'NNP'),\n",
       " ('--', ':'),\n",
       " ('In', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('wake', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('string', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('abuses', 'NNS')]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "42b48332-9b2e-4b4e-9854-18b44381c541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPE WASHINGTON\n",
      "GPE New York\n",
      "PERSON Loretta E. Lynch\n",
      "GPE Brooklyn\n"
     ]
    }
   ],
   "source": [
    "for chunk in nltk.ne_chunk(tagged):\n",
    "     #print(chunk)\n",
    "     if hasattr(chunk, 'label'): #  Checks if the chunk is a named entity (i.e., has a label).\n",
    "        print(chunk.label(), ' '.join(c[0] for c in chunk)) #(e.g., PERSON, GPE for location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e52db26-bc3d-48ab-9172-f1e76b8ce6f5",
   "metadata": {},
   "source": [
    "<b>\n",
    "<a id='Text Normalizaion ( Cleaning )'></a>\n",
    "<font size=\"5\">Text Normalizaion ( Cleaning )</font>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8b024d-00a5-4735-b1d9-b24f1cbaa512",
   "metadata": {},
   "source": [
    "# How to remove all this from the text?\n",
    "- **Stop Words** : (such as “the”, “a”, “an”, “in”) -- a search engine has been programmed to ignore.\n",
    "- **Punctuation** : ['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',....]\n",
    "- **Stemming** : Stemming reduce the text by a set of pre-defined rules like removing `ing` from verbs\n",
    "    - Stemming is the process of producing morphological variants of a root/base word.\n",
    "    - اداه تسمح بتجريد اى كلمه من جميع الاضافات التى فيها والعوده للمصدر الاصلى لها \n",
    "    - Example : (plays,played,playing,player) -->> play \n",
    "- **Lemmatization** : reduce the word by looking it up in the `WordNet` where it tries to find the root of the word for example `rocks` -> `rock`\n",
    "    - Lemmatization is similar to stemming but it brings context to the words.\n",
    "    - So it links words with similar meanings to one word.\n",
    "    - اكثر قوة وفعاليه لانها مش بتكتفى انها تحذف الزوائد من الكلمات ولكن بتبحث ف معنى واصل الكلمه\n",
    "    - Example : ( rocks - rock ) , ( corpora - corpus ) , ( better - good )\n",
    "#### Lemmatization vs Stemming\n",
    " - The key concept here is that stemming sometime destroy the word unlike lemmatization where we keep the meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e3edb8",
   "metadata": {},
   "source": [
    "<b>\n",
    "<a id='stop words in EN'></a>\n",
    "<font size=\"5\">stop words in EN</font>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "172ae06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK(Natural Language Toolkit) in python has a list of stopwords stored in 16 different languages\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1fdc93ee-e2de-4e04-ab10-ef4b991d0902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec88062-8c39-42e7-8194-6771f9942e65",
   "metadata": {},
   "source": [
    "## Task 1 :  Removing stop words with NLTK ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753ee9c-30b2-4ab1-bcf4-2de7e2264b3d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a5a9d7-b3ff-4ee7-964a-2817421c0e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d79c72-5677-40d1-88b8-3c84df1a2aff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b367436-1437-40fb-ad5a-3dbdee80912c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f5320f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n",
      "sample sentence , showing stop words filtration .\n"
     ]
    }
   ],
   "source": [
    "example_sent = \"\"\"This is a sample sentence, showing off the stop words filtration.\"\"\"\n",
    "stop_words = stopwords.words('english')\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "filtered_sentence = []  \n",
    "for w in word_tokens:\n",
    "    if w.lower() not in stop_words:\n",
    "        filtered_sentence.append(w.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bf645d",
   "metadata": {},
   "source": [
    "<b>\n",
    "<a id='stop words in AR'></a>\n",
    "<font size=\"5\">stop words in AR</font>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1f495a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701\n"
     ]
    }
   ],
   "source": [
    "stop_word_arabic = set(stopwords.words(\"arabic\"))\n",
    "print(len(stop_word_arabic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1cefcf50-95e7-4352-81ed-8c48a8a2c929",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'حار - عل - لن - إياكما - لكن - ت - خاصة - لي - الآن - سبت - بعدا - حدَث - تموز - قام - اثنان - مرّة - أمامكَ - لنا - جير - عَدَسْ - كأنما - ثلاثون - فيه - لاسيما - صباح - إذما - أيّ - إياه - هللة - هَذا - أول - ة - لها - أُفٍّ - ستمئة - طاء - هاء - فلان - نوفمبر - تانِك - سمعا - بهما - بل - هنالك - استحال - إياكم - بين - عليه - هَيْهات - مكانكنّ - إنَّ - تخذ - لكم - لولا - سبعون - فلس - كسا - نفس - أمّا - هيت - غدا - بطآن - حمدا - نحن - راح - همزة - ص - كلما - ومن - أطعم - كاف - كل - ثلاثمائة - ثالث - فرادى - طاق - أن - مذ - ذين - أنتن - جلل - غالبا - ذو - دونك - تلقاء - ما برح - ذينك - قد - لعل - ش - أضحى - كأي - وَيْ - يورو - هَذِه - أيار - أيلول - ع - تفعلان - أين - حجا - كذلك - بهن - هيّا - خاء - ليسوا - شين - ثمّة - أعطى - ثمانون - تبدّل - ثمانين - علق - طالما - هناك - جعل - نحو - بنا - شَتَّانَ - هاهنا - أربع - قاطبة - أل - بئس - هلّا - ذواتي - أمام - عاد - أخذ - لستما - أسكن - أمس - آب - مادام - ذواتا - حين - رُبَّ - فوق - انقلب - مائة - لبيك - غداة - حمٌ - كلَّا - خميس - تلكما - أقبل - ولا - لوما - سادس - لكما - كيت - مئتان - علم - هذا - ثمة - ماي - مما - ست - إياكن - ستة - أيا - عشر - ذلكم - كما - أف - اللاتي - أخٌ - نا - ّأيّان - حمو - كاد - أبريل - إليكما - جمعة - نون - تسعمائة - صدقا - ثمانية - هن - لهن - إيهٍ - أينما - يفعلان - وما - هَاتِه - وجد - اربعين - ذاك - ن - س - صهٍ - كأيّ - فيها - ليرة - يوليو - سبع - ثماني - اللتيا - ى - به - ألف - تارة - إليكن - كيفما - ما أفعله - كأنّ - وهو - واهاً - كثيرا - ذات - سقى - بك - يونيو - ج - اللذان - تعلَّم - هذه - لعلَّ - نَّ - قبل - فو - نَخْ - أنًّ - منه - كن - كأين - ذلكما - ليس - ثمان - حقا - أو - أغسطس - أهلا - هاتي - فبراير - أمامك - على - دون - كذا - مساء - شيكل - ز - مازال - لم - ي - بما - آهٍ - كان - أى - رزق - فإذا - إما - والذي - خ - اللتين - وراءَك - هؤلاء - أرى - درى - أمد - ألا - زعم - إزاء - إليكَ - غير - إلّا - ثلاثمئة - لستم - تسع - لسنا - تفعلون - ث - مكانكم - سوف - اثنين - ضاد - تلك - أيضا - هما - فاء - وإذ - صاد - بعد - حاشا - دواليك - هي - ثمّ - ذ - جيم - تي - إنما - وهب - اثني - أنشأ - إياك - زاي - هَذانِ - هذي - هيا - فيفري - بهم - أجل - إيانا - كم - بات - ح - بكن - عشرة - ذهب - درهم - طَق - تلكم - تسعون - هاكَ - بؤسا - بخٍ - كليهما - يفعلون - بها - ك - إن - شرع - أنى - خمس - ذلك - د - بعض - حيثما - يمين - أوت - ريث - كلّما - إذاً - إذ - قطّ - ذانك - فمن - ذال - ثلاثين - مافتئ - لست - لكنما - لكي - ثاني - فيم - لكنَّ - بي - من - هنا - تعسا - إذن - أكثر - ماذا - أفعل به - جوان - الألى - ليت - عدَّ - سبتمبر - عن - هَاتِي - قرش - سوى - هيهات - إياهما - هلم - لهم - لئن - ؤ - أي - بكم - عشرون - آض - أوّهْ - عيانا - حيث - انبرى - آذار - ما انفك - ترك - أخو - حَذارِ - إياي - هَجْ - آه - لستن - أيها - إليك - بيد - منذ - ثم - غادر - طرا - ستمائة - يا - أيّان - إليكنّ - أربعمائة - خلافا - عليك - نيف - طفق - واحد - ف - حبذا - ديسمبر - لو - في - هاك - بضع - حبيب - بَلْهَ - تاسع - ء - ذه - خلا - لام - مهما - نعم - أنّى - ستين - سبحان - جميع - مايو - تِي - مئة - وا - قلما - كرب - حسب - كأن - تِه - وإن - معاذ - بسّ - غين - لهما - ذلكن - سبعمائة - ممن - إنا - مليم - ثلاث - سرا - اللتان - مكانكما - خلف - ورد - ضحوة - ظ - عامة - كلاهما - كلا - واو - أفٍّ - فلا - إنه - له - ليستا - صبر - أوه - ذَيْنِ - آهِ - مه - ءَ - ظنَّ - كى - إلى - ذان - فإن - والذين - تحت - ثلاثاء - أصبح - حزيران - صار - عاشر - ساء - ذِي - خمسمائة - ثامن - أمسى - باء - بكما - راء - وُشْكَانَ - مثل - ولكن - أربعاء - آنفا - لدن - اللائي - جانفي - دال - ذانِ - الألاء - هو - رأى - لسن - ذِه - لما - حاي - شبه - بَسْ - حتى - كيف - كليكما - تشرين - ما - هذين - هَاتانِ - ته - كأيّن - أنتما - سنتيم - آناء - ذوا - أبو - كِخ - عند - تسعين - تاء - إليكم - بماذا - ظاء - سبعة - أقل - أنا - أربعة - تفعلين - عما - ثلاثة - خمسين - أكتوبر - عشرين - إحدى - هَؤلاء - آي - عين - مارس - سين - بس - هَذَيْنِ - ثمانمئة - ابتدأ - ريال - م - أفريل - أنبأ - ستون - عوض - فضلا - ميم - سرعان - لعمر - صهْ - أولاء - ط - مكانَك - أبدا - متى - حادي - ياء - بغتة - صراحة - رابع - جنيه - هَذِي - رويدك - ض - ل - لا - لك - آمينَ - ذيت - صبرا - ها - إياهن - ليسا - آها - خمسون - إمّا - أجمع - خبَّر - ليست - تين - تسعمئة - كانون - ارتدّ - خامس - الذين - سبعمئة - أنت - اثنا - عدا - ا - ولو - ثان - هاتين - يناير - منها - دينار - قاف - ر - حرى - ألفى - إلَيْكَ - شتانَ - هاتان - بلى - أبٌ - علًّ - بمن - سبعين - لا سيما - جويلية - كلتا - بخ - الذي - و - إياها - يوان - خمسة - آ - اربعون - أولئك - نبَّا - إي - فيما - ثاء - شباط - لمّا - لدى - مع - أما - أم - حاء - أ - أنتِ - نيسان - أولالك - خمسمئة - التي - هلا - ئ - تَيْنِ - رجع - دولار - إذا - سابع - هبّ - ذا - أحد - ذي - أربعمئة - أصلا - أعلم - وإذا - تانِ - اللذين - أنتم - هكذا - ب - اللواتي - تجاه - هم - إلا - عسى - غ - هَاتَيْنِ - إى - شمال - ين - حيَّ - آهاً - لكيلا - شتان - هاته - تحوّل - خال - اخلولق - ه - أخبر - لات - كي - تسعة - ثمَّ - هذان - عجبا - أوشك - سحقا - إياهم - ظلّ - اتخذ - ثمنمئة - زود - تينك - إيه - ق - هل'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' - '.join(stop_word_arabic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4d8a0b",
   "metadata": {},
   "source": [
    "<b>\n",
    "<a id='punctuation'></a>\n",
    "<font size=\"5\">punctuation</font>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0de2eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "\n",
    "punctuation = list(punctuation)\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e398a0-dd3f-4f48-9807-ae301a6d62f2",
   "metadata": {},
   "source": [
    "## Task 2 : Removing punctuation with tokenization ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a901f15-ba43-4733-822e-c58254385393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97aff8e-aaae-48ed-8910-4e346dcf253f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdad3af4-ac39-4924-8d04-8de56b58a253",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c2c2ad-5a9c-4e5e-a345-f610799e90cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75111fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['My', 'Email', 'address', 'is', ':', 'taneshbalodi8', '@', 'gmail.com', '.']\n",
      "['My', 'Email', 'address', 'is', 'taneshbalodi8', 'gmail.com']\n"
     ]
    }
   ],
   "source": [
    "example_sent = \"\"\"My Email address is: taneshbalodi8@gmail.com.\"\"\"\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "filtered_sentence = []\n",
    "  \n",
    "for w in word_tokens:\n",
    "    if w not in punctuation:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6926953f-8d40-4bb9-b09d-9c3ede5764c9",
   "metadata": {},
   "source": [
    "## Task 3 : Removing punctuation without tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3504c80e-13db-4635-a2d2-9dfffd8e88dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc88dd8-fd9a-4558-aba4-c91fc4ef0b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bf9b23-6786-49a8-b686-54c03a09014b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b43893-61c8-4c56-b3d4-9275bf31f3e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc77a47b-ce8c-4a44-88d6-89395e2bcb06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a3c02da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Email address is: hossamfares180100@gmail.com.\n",
      "['M', 'y', ' ', 'E', 'm', 'a', 'i', 'l', ' ', 'a', 'd', 'd', 'r', 'e', 's', 's', ' ', 'i', 's', ' ', 'h', 'o', 's', 's', 'a', 'm', 'f', 'a', 'r', 'e', 's', '1', '8', '0', '1', '0', '0', 'g', 'm', 'a', 'i', 'l', 'c', 'o', 'm']\n",
      "My Email address is hossamfares180100gmailcom\n"
     ]
    }
   ],
   "source": [
    "example_sent = \"\"\"My Email address is: hossamfares180100@gmail.com.\"\"\"\n",
    "filtered_sentence = []\n",
    "  \n",
    "for w in example_sent:\n",
    "    if w not in punctuation:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(example_sent)\n",
    "print(filtered_sentence)\n",
    "print(''.join(filtered_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a0ad68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68cf89ea",
   "metadata": {},
   "source": [
    "<b>\n",
    "<a id='stemming using nltk'></a>\n",
    "<font size=\"5\">stemming using nltk</font>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f52dc997",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program  :  program\n",
      "programs  :  program\n",
      "programmer  :  programm\n",
      "programming  :  program\n",
      "programmers  :  programm\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "  \n",
    "ps = PorterStemmer()\n",
    "  \n",
    "# choose some words to be stemmed\n",
    "words = [\"program\", \"programs\", \"programmer\", \"programming\", \"programmers\"]\n",
    "  \n",
    "for w in words:\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "47dedf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Programmers  :  programm\n",
      "program  :  program\n",
      "with  :  with\n",
      "programming  :  program\n",
      "languages  :  languag\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Programmers program with programming languages\"\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "\n",
    "for w in words:\n",
    "    print(w, \" : \", ps.stem(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2b420f-8b47-4a81-8741-c1b56ad9e494",
   "metadata": {},
   "source": [
    "###  HINT: snowballstemmer is somewhat faster and more logical than the original Porter Stemmer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c7fab57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous ---> generous\n",
      "generate ---> generat\n",
      "generously ---> generous\n",
      "generation ---> generat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "snowball = SnowballStemmer(language='english')\n",
    "words = ['generous','generate','generously','generation']\n",
    "for word in words:\n",
    "    print(word,\"--->\",snowball.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dad0e74-6802-4872-b88a-312a7236d4ab",
   "metadata": {},
   "source": [
    "# Snowball stemmer support different languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "00857f36-fb8c-4bd7-8673-80a74969c62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arabic, danish, dutch, english, finnish, french, german, hungarian, italian, norwegian, porter, portuguese, romanian, russian, spanish, swedish\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "print(\", \".join(SnowballStemmer.languages))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f156a9",
   "metadata": {},
   "source": [
    "<b>\n",
    "<a id='stemming in AR'></a>\n",
    "<font size=\"5\">stemming in AR</font>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6643e7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الجري --> الجري\n",
      "تجري --> تجري\n",
      "يجرون --> يجرون\n",
      "جري --> جري\n",
      "يجري --> يجري\n"
     ]
    }
   ],
   "source": [
    "#stemming is very weak in arabic\n",
    "words = ['الجري','تجري','يجرون','جري','يجري']\n",
    "for word in words:\n",
    "    print(word+' --> '+ ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60c12266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الجري --> الجر\n",
      "تجري --> تجر\n",
      "يجرون --> يجرو\n",
      "جري --> جر\n",
      "يجري --> يجر\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(language='arabic')\n",
    "\n",
    "words = ['الجري','تجري','يجرون','جري','يجري']\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8ad08e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "جري\n",
      "تجر\n",
      "يجر\n",
      "جري\n",
      "يجر\n",
      "علم\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.isri import ISRIStemmer\n",
    "st = ISRIStemmer()\n",
    "words = ['الجري','تجري','يجرون','جري','يجري']\n",
    "\n",
    "for word in words : \n",
    "    print(st.stem(word))\n",
    "print (st.stem('اعلاميون'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a167dc33",
   "metadata": {},
   "source": [
    "<b>\n",
    "<a id='lemmatization using nltk'></a>\n",
    "<font size=\"5\">Lemmatization using nltk</font>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f76aa1-39d6-4051-a0bc-73387bc9dbad",
   "metadata": {},
   "source": [
    "- Examples of lemmatization:\n",
    "    - rocks : rock\n",
    "    - corpora : corpus\n",
    "    - better : good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "72c884e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "65c77536-e4b1-41cc-a6fc-048190de7ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocks : rock\n",
      "corpora : corpus\n",
      "better : good\n"
     ]
    }
   ],
   "source": [
    "print(\"rocks :\", lemmatizer.lemmatize(\"rocks\"))\n",
    "print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))\n",
    "print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "193036de-f239-4a9b-b4be-f5a5c659d38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better\n",
      "good\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Without POS (default is noun)\n",
    "print(lemmatizer.lemmatize(\"better\"))\n",
    "# With POS as adjective ('a')\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))  \n",
    "# With POS as verb ('v')\n",
    "print(lemmatizer.lemmatize(\"running\", pos=\"v\"))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dd4b2a4b-d8bf-47e4-a34a-d4a91806409b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kites ---> kite\n",
      "babies ---> baby\n",
      "dogs ---> dog\n",
      "flying ---> flying\n",
      "smiling ---> smiling\n",
      "driving ---> driving\n",
      "died ---> died\n",
      "tried ---> tried\n",
      "feet ---> foot\n"
     ]
    }
   ],
   "source": [
    "# single word lemmatization examples\n",
    "list1 = ['kites', 'babies', 'dogs', 'flying', 'smiling', 'driving', 'died', 'tried', 'feet']\n",
    "\n",
    "for words in list1:\n",
    "    print(words + \" ---> \" + lemmatizer.lemmatize(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6ed8186",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is\n",
      "wa\n",
      "be\n",
      "been\n",
      "are\n",
      "were\n"
     ]
    }
   ],
   "source": [
    "words = [\"is\",\"was\",\"be\",\"been\",\"are\",\"were\"]\n",
    "\n",
    "for word in words : \n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2fa911ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\n",
      "be\n",
      "be\n",
      "be\n",
      "be\n",
      "be\n"
     ]
    }
   ],
   "source": [
    "#what if we put them as verb\n",
    "words = [\"is\",\"was\",\"be\",\"been\",\"are\",\"were\"]\n",
    "for word in words : \n",
    "    print(lemmatizer.lemmatize(word,'v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0542e416",
   "metadata": {},
   "source": [
    "<b>\n",
    "<a id='lemmatization in AR'></a>\n",
    "<font size=\"5\">lemmatization in AR</font>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63175bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "الجري\n",
      "تجري\n",
      "يجرون\n",
      "جري\n",
      "يجري\n"
     ]
    }
   ],
   "source": [
    "words = ['الجري','تجري','يجرون','جري','يجري']\n",
    "\n",
    "for word in words : \n",
    "    print(lemmatizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34e1c37-a118-4645-9fbe-6f03a3a9efa3",
   "metadata": {},
   "source": [
    "### Task 4 : Create Method Take (Text as input ) then : ( 15 MIN )\n",
    "-  returns a cleaned version by removing stopwords, punctuation, applying lemmatization, and stemming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7747df60-91db-46f2-b4ea-a265ff97248e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa3d232-3687-42f0-9997-f0afd913c237",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97e72b5-7bea-4a0b-966b-b5df1f7995cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07213690-d46e-4563-9c10-d77d29290a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137dea9c-07bd-48a9-9a67-d1cf1d878db9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d32d1d-9bd8-4d10-ba5e-ed7984e881e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a856e9-a4f4-49f0-921b-4c58e3dab550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "30745643-a4f1-4e70-9d78-d908805f4b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    filtered_sentence = []  \n",
    "    \n",
    "    for word in text.split(\" \"):\n",
    "        if word.lower() not in stop_words and  w not in punctuation :\n",
    "            filtered_sentence.append(word.lower())\n",
    "    #print(filtered_sentence)\n",
    "    \n",
    "    ps_text = [ps.stem(word) for word in filtered_sentence]\n",
    "    lemmatizer_text = [lemmatizer.lemmatize(word) for word in filtered_sentence]\n",
    "    return ps_text , lemmatizer_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "155c3f4d-f327-4638-ae43-8d319ae33c83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['welcom',\n",
       "  'nlp',\n",
       "  'course!.',\n",
       "  'natur',\n",
       "  'languag',\n",
       "  'process',\n",
       "  '(nlp)',\n",
       "  'excit',\n",
       "  'field',\n",
       "  'artifici',\n",
       "  'intellig',\n",
       "  'enabl',\n",
       "  'comput',\n",
       "  'understand,',\n",
       "  'interpret,',\n",
       "  'gener',\n",
       "  'human',\n",
       "  'language.'],\n",
       " ['welcome',\n",
       "  'nlp',\n",
       "  'course!.',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  '(nlp)',\n",
       "  'exciting',\n",
       "  'field',\n",
       "  'artificial',\n",
       "  'intelligence',\n",
       "  'enables',\n",
       "  'computer',\n",
       "  'understand,',\n",
       "  'interpret,',\n",
       "  'generate',\n",
       "  'human',\n",
       "  'language.'])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"Welcome to the NLP Course!. Natural Language Processing (NLP) is an exciting field of Artificial Intelligence that enables computers to understand, interpret, and generate human language.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b9dc3b-c2e5-4fe3-8004-5baad1080942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78911679-6a2f-482f-85e2-4ce127624c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f77cc0f-21ed-4098-8de6-73b926f43f04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80b1e853-fe56-40e8-b442-84aac28d7330",
   "metadata": {},
   "source": [
    "# Extra Topics :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7a46d4-71b9-416c-b7e5-a5fbe1234132",
   "metadata": {},
   "source": [
    "## Some works in Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cc535da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting qalsadi\n",
      "  Downloading qalsadi-0.5-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting Arabic-Stopwords>=0.4.2 (from qalsadi)\n",
      "  Downloading Arabic_Stopwords-0.4.3-py3-none-any.whl.metadata (8.9 kB)\n",
      "Collecting alyahmor>=0.2 (from qalsadi)\n",
      "  Downloading alyahmor-0.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting arramooz-pysqlite>=0.4.2 (from qalsadi)\n",
      "  Downloading arramooz_pysqlite-0.4.2-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting codernitydb3 (from qalsadi)\n",
      "  Downloading codernitydb3-0.6.0.tar.gz (46 kB)\n",
      "     ---------------------------------------- 0.0/46.1 kB ? eta -:--:--\n",
      "     -------------------------- ------------- 30.7/46.1 kB 1.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 46.1/46.1 kB 568.6 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting libqutrub>=1.2.3 (from qalsadi)\n",
      "  Downloading libqutrub-1.2.4.1-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting mysam-tagmanager>=0.3.3 (from qalsadi)\n",
      "  Downloading mysam_tagmanager-0.4-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting naftawayh>=0.3 (from qalsadi)\n",
      "  Downloading Naftawayh-0.4-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting pickledb>=0.9.2 (from qalsadi)\n",
      "  Downloading pickledb-1.3.2.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting pyarabic>=0.6.7 (from qalsadi)\n",
      "  Downloading PyArabic-0.6.15-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting tashaphyne>=0.3.4.1 (from qalsadi)\n",
      "  Downloading Tashaphyne-0.3.6-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: orjson in c:\\users\\fady\\anaconda3\\lib\\site-packages (from pickledb>=0.9.2->qalsadi) (3.10.5)\n",
      "Collecting aiofiles (from pickledb>=0.9.2->qalsadi)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\fady\\anaconda3\\lib\\site-packages (from pyarabic>=0.6.7->qalsadi) (1.16.0)\n",
      "Downloading qalsadi-0.5-py3-none-any.whl (264 kB)\n",
      "   ---------------------------------------- 0.0/264.3 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 81.9/264.3 kB 2.2 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 174.1/264.3 kB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 256.0/264.3 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 264.3/264.3 kB 1.8 MB/s eta 0:00:00\n",
      "Downloading alyahmor-0.2-py3-none-any.whl (65 kB)\n",
      "   ---------------------------------------- 0.0/65.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 65.1/65.1 kB 1.2 MB/s eta 0:00:00\n",
      "Downloading Arabic_Stopwords-0.4.3-py3-none-any.whl (360 kB)\n",
      "   ---------------------------------------- 0.0/360.5 kB ? eta -:--:--\n",
      "   ------------ --------------------------- 112.6/360.5 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 204.8/360.5 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 307.2/360.5 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 360.5/360.5 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading arramooz_pysqlite-0.4.2-py3-none-any.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/6.3 MB 2.3 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.2/6.3 MB 2.1 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.3/6.3 MB 2.2 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.4/6.3 MB 2.2 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.5/6.3 MB 2.4 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.6/6.3 MB 2.2 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 0.7/6.3 MB 2.3 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.8/6.3 MB 2.3 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.9/6.3 MB 2.3 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.0/6.3 MB 2.3 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.1/6.3 MB 2.3 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.2/6.3 MB 2.3 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.3/6.3 MB 2.3 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 1.4/6.3 MB 2.2 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 1.5/6.3 MB 2.3 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 1.6/6.3 MB 2.3 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 2.3 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 1.9/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 2.0/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.1/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 2.2/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   -------------- ------------------------- 2.3/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.4/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 2.5/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 2.7/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 2.8/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 2.9/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.0/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 3.1/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 3.2/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.3/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 3.4/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.5/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 3.6/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 3.8/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 3.9/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 4.0/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.0/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.0/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.0/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.0/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.0/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.0/6.3 MB 2.3 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 4.1/6.3 MB 2.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 4.8/6.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 4.9/6.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 5.0/6.3 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.1/6.3 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.2/6.3 MB 2.3 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.3/6.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.4/6.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 5.6/6.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.6/6.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.9/6.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 6.0/6.3 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.1/6.3 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.2/6.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.3/6.3 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 2.2 MB/s eta 0:00:00\n",
      "Downloading libqutrub-1.2.4.1-py3-none-any.whl (138 kB)\n",
      "   ---------------------------------------- 0.0/139.0 kB ? eta -:--:--\n",
      "   -------------------------- ------------- 92.2/139.0 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 139.0/139.0 kB 2.0 MB/s eta 0:00:00\n",
      "Downloading mysam_tagmanager-0.4-py3-none-any.whl (37 kB)\n",
      "Downloading Naftawayh-0.4-py3-none-any.whl (332 kB)\n",
      "   ---------------------------------------- 0.0/332.6 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 92.2/332.6 kB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 204.8/332.6 kB 2.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 317.4/332.6 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 332.6/332.6 kB 2.1 MB/s eta 0:00:00\n",
      "Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
      "   ---------------------------------------- 0.0/126.4 kB ? eta -:--:--\n",
      "   ----------------------------- ---------- 92.2/126.4 kB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 126.4/126.4 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading Tashaphyne-0.3.6-py3-none-any.whl (251 kB)\n",
      "   ---------------------------------------- 0.0/251.5 kB ? eta -:--:--\n",
      "   ----------------- ---------------------- 112.6/251.5 kB 2.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 204.8/251.5 kB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 251.5/251.5 kB 2.2 MB/s eta 0:00:00\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Building wheels for collected packages: pickledb, codernitydb3\n",
      "  Building wheel for pickledb (setup.py): started\n",
      "  Building wheel for pickledb (setup.py): finished with status 'done'\n",
      "  Created wheel for pickledb: filename=pickleDB-1.3.2-py3-none-any.whl size=5307 sha256=104132bbb93fc846574c9be4c58e42d679a1fbbf1a9850ea2a9bd3a3f1bff7e3\n",
      "  Stored in directory: c:\\users\\fady\\appdata\\local\\pip\\cache\\wheels\\ad\\e2\\59\\dbb52181bcb1551e623e55c2a715add67a2fdbca7d62632da8\n",
      "  Building wheel for codernitydb3 (setup.py): started\n",
      "  Building wheel for codernitydb3 (setup.py): finished with status 'done'\n",
      "  Created wheel for codernitydb3: filename=codernitydb3-0.6.0-py3-none-any.whl size=59877 sha256=4ce7e2e44fbe5f3a8d65b74a8392b4ffc456975e4be3e6a6cc7e4bbeaacdae8e\n",
      "  Stored in directory: c:\\users\\fady\\appdata\\local\\pip\\cache\\wheels\\10\\46\\58\\dbd81132e05ee2236e2a7ff9d8a7ec9d00dcfc92dd1e9e6998\n",
      "Successfully built pickledb codernitydb3\n",
      "Installing collected packages: mysam-tagmanager, pyarabic, codernitydb3, aiofiles, tashaphyne, pickledb, libqutrub, arramooz-pysqlite, Arabic-Stopwords, naftawayh, alyahmor, qalsadi\n",
      "Successfully installed Arabic-Stopwords-0.4.3 aiofiles-24.1.0 alyahmor-0.2 arramooz-pysqlite-0.4.2 codernitydb3-0.6.0 libqutrub-1.2.4.1 mysam-tagmanager-0.4 naftawayh-0.4 pickledb-1.3.2 pyarabic-0.6.15 qalsadi-0.5 tashaphyne-0.3.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install qalsadi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "88619361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "جرة تجرة جرى جرة جرى \n",
      "يقل يقل تقل قول\n"
     ]
    }
   ],
   "source": [
    "import qalsadi.lemmatizer\n",
    "lemmer = qalsadi.lemmatizer.Lemmatizer()\n",
    "words = ['الجري','تجري','يجرون','جري','يجري']\n",
    "\n",
    "for word in words : \n",
    "    print(lemmer.lemmatize(word),end=' ')\n",
    "print()\n",
    "print (st.stem('يقول'),end=' ')\n",
    "print (st.stem('يقولون'),end=' ')\n",
    "print (st.stem('تقول'),end=' ')\n",
    "print (st.stem('مقوله'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec964e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['هل', 'احتاج', 'إلى', 'ترجمة', 'كي', 'تف', 'خطاب', 'ملك', '؟', 'لغة', '\"', 'كلاسيكي', '\"(', 'فصحى', ')', 'موجود', 'في', 'كل', 'لغة', 'كذلك', 'لغة', '\"', 'دارج', '\"..', 'فرنسة', 'التي', 'درس', 'في', 'مدرس', 'ليست', 'فرنسة', 'التي', 'استخدم', 'ناس', 'في', 'شوارع', 'باريس', '..', 'ملك', 'بريطاني', 'لا', 'خطب', 'بلغة', 'شوارع', 'أدان', '..', 'كل', 'مقام', 'مقال']\n"
     ]
    }
   ],
   "source": [
    "text = text = \"\"\"هل تحتاج إلى ترجمة كي تفهم خطاب الملك؟ اللغة \"الكلاسيكية\" (الفصحى) موجودة في كل اللغات وكذلك اللغة \"الدارجة\" .. الفرنسية التي ندرس في المدرسة ليست الفرنسية التي يستخدمها الناس في شوارع باريس .. وملكة بريطانيا لا تخطب بلغة شوارع لندن .. لكل مقام مقال\"\"\"\n",
    "lemmas = lemmer.lemmatize_text(text)\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8fe57d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('هل', 'stopword'), ('احتاج', 'verb'), ('إلى', 'stopword'), ('ترجمة', 'noun'), ('كي', 'stopword'), ('تف', 'noun'), ('خطاب', 'noun'), ('ملك', 'noun'), ('؟', 'pounct'), ('لغة', 'noun'), ('\"', 'pounct'), ('كلاسيكي', 'noun'), ('\"(', 'pounct'), ('فصحى', 'noun'), (')', 'pounct'), ('موجود', 'noun'), ('في', 'stopword'), ('كل', 'stopword'), ('لغة', 'noun'), ('كذلك', 'stopword'), ('لغة', 'noun'), ('\"', 'pounct'), ('دارج', 'noun'), ('\"..', 'pounct'), ('فرنسة', 'noun'), ('التي', 'stopword'), ('درس', 'verb'), ('في', 'stopword'), ('مدرس', 'noun'), ('ليست', 'stopword'), ('فرنسة', 'noun'), ('التي', 'stopword'), ('استخدم', 'verb'), ('ناس', 'noun'), ('في', 'stopword'), ('شوارع', 'noun'), ('باريس', 'all'), ('..', 'pounct'), ('ملك', 'noun'), ('بريطاني', 'noun'), ('لا', 'stopword'), ('خطب', 'verb'), ('بلغة', 'noun'), ('شوارع', 'noun'), ('أدان', 'verb'), ('..', 'pounct'), ('كل', 'stopword'), ('مقام', 'noun'), ('مقال', 'noun')]\n"
     ]
    }
   ],
   "source": [
    "lemmas = lemmer.lemmatize_text(text, return_pos=True)\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1143a9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['هَلْ', 'اِحْتَاجَ', 'إِلَى', 'تَرْجَمَةٌ', 'كَيْ', 'تَفَهُّمٌ', 'خَطَّابٌ', 'مَلَكٌ', '؟', 'لُغَةٌ', '\"', 'كِلاَسِيكِيٌّ', '\"(', 'فُصْحَى', ')', 'مَوْجُودٌ', 'فِي', 'كُلَّ', 'لُغَةٌ', 'كَذَلِكَ', 'لُغَةٌ', '\"', 'دَارِجٌ', '\"..', 'فَرَنْسِيّ', 'الَّتِي', 'دَرَسَ', 'فِي', 'مَدْرَسَةٌ', 'لَيْسَتْ', 'فَرَنْسِيّ', 'الَّتِي', 'اِسْتَخْدَمَ', 'نَاسٌ', 'فِي', 'شَوَارِعٌ', 'باريس', '..', 'مَلَكٌ', 'برِيطانِيا', 'لَا', 'خَطَبَ', 'بَلَغَةٌ', 'شَوَارِعٌ', 'أَدَانَ', '..', 'كُلَّ', 'مَقَامٌ', 'مَقَالٌ']\n"
     ]
    }
   ],
   "source": [
    "# put diacritics on text\n",
    "lemmer.set_vocalized_lemma()\n",
    "lemmas = lemmer.lemmatize_text(text)\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd89aaa-8644-4df7-8d21-a80089808a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb7241-c8d8-47e1-87ce-f54f22329e89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cb9b37-08ee-4a82-97d1-ea113492124f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c8b057-da03-4105-bddc-f2a579aa0c31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98071de7-d8c7-4f90-be29-c64358457d3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c89f0148",
   "metadata": {},
   "source": [
    "<b>\n",
    "<a id='special arabic cleaning functions'></a>\n",
    "<font size=\"5\">special arabic cleaning functions</font>\n",
    "</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0bc8dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "هَلْ اِحْتَاجَ إِلَى تَرْجَمَةٌ كَيْ تَفَهُّمٌ خَطَّابٌ مَلَكٌ ؟ لُغَةٌ \" كِلاَسِيكِيٌّ \"( فُصْحَى ) مَوْجُودٌ فِي كُلَّ لُغَةٌ كَذَلِكَ لُغَةٌ \" دَارِجٌ \".. فَرَنْسِيّ الَّتِي دَرَسَ فِي مَدْرَسَةٌ لَيْسَتْ فَرَنْسِيّ الَّتِي اِسْتَخْدَمَ نَاسٌ فِي شَوَارِعٌ باريس .. مَلَكٌ برِيطانِيا لَا خَطَبَ بَلَغَةٌ شَوَارِعٌ أَدَانَ .. كُلَّ مَقَامٌ مَقَالٌ\n",
      "-----------------------------------------------------\n",
      "هل احتاج إلى ترجمة كي تفهم خطاب ملك ؟ لغة \" كلاسيكي \"( فصحى ) موجود في كل لغة كذلك لغة \" دارج \".. فرنسي التي درس في مدرسة ليست فرنسي التي استخدم ناس في شوارع باريس .. ملك بريطانيا لا خطب بلغة شوارع أدان .. كل مقام مقال\n",
      "-----------------------------------------------------\n",
      "هل احتاج الي ترجمه كي تفهم خطاب ملك ؟ لغه \" كلاسيكي \"( فصحي ) موجود في كل لغه كذلك لغه \" دارج \".. فرنسي التي درس في مدرسه ليست فرنسي التي استخدم ناس في شوارع باريس .. ملك بريطانيا لا خطب بلغه شوارع ادان .. كل مقام مقال\n"
     ]
    }
   ],
   "source": [
    "# special cleaning to arabic text\n",
    "import re\n",
    "arabic_diacritics = re.compile(\"\"\"\n",
    "                             ّ    | # Tashdid\n",
    "                             َ    | # Fatha\n",
    "                             ً    | # Tanwin Fath\n",
    "                             ُ    | # Damma\n",
    "                             ٌ    | # Tanwin Damm\n",
    "                             ِ    | # Kasra\n",
    "                             ٍ    | # Tanwin Kasr\n",
    "                             ْ    | # Sukun\n",
    "                             ـ     # Tatwil/Kashida\n",
    "                         \"\"\", re.VERBOSE)\n",
    "\n",
    "\n",
    "def normalize_arabic(text):\n",
    "    text = re.sub(\"[إأآا]\", \"ا\", text)\n",
    "    text = re.sub(\"ى\", \"ي\", text)\n",
    "    text = re.sub(\"ؤ\", \"ء\", text)\n",
    "    text = re.sub(\"ئ\", \"ء\", text)\n",
    "    text = re.sub(\"ة\", \"ه\", text)\n",
    "    text = re.sub(\"گ\", \"ك\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_diacritics(text):\n",
    "    text = re.sub(arabic_diacritics, '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_repeating_char(text):\n",
    "    return re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "print(' '.join(lemmas))\n",
    "print('-----------------------------------------------------')\n",
    "print(remove_diacritics(' '.join(lemmas)))\n",
    "print('-----------------------------------------------------')\n",
    "print(normalize_arabic(remove_diacritics(' '.join(lemmas))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d6ead5-5db6-4542-8913-458b2fc3c390",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
